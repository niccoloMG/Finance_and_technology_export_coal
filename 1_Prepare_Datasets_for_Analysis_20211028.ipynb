{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation to this script we manually matched transactions and units. <br>\n",
    "This script cleans the resulting datasets, prepares them for the analysis and merges them into one final dataset. <br>\n",
    "In the end we will get one dataset with only unique units and one with unit-transactions.<br>\n",
    "Each of the two will contain information on finance and on other involved actors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Import and prepare the manually matched datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manually matched the financing data from urgewald and GEM with units. Both are imported and merged in the following. The resulting dataset is prepared for the final merging with the Platts data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pz = '2021_01' #date of GEM plant tracker\n",
    "fz = '2020_12' #date of GEM finance tracker\n",
    "uz = '2021_02' #date of urgewald finance data for coal exit list, received per mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import WEPP platts data\n",
    "tz = '2019' #date of newest platts\n",
    "df_platts_2017 = pd.read_csv('.//tables//2017_df_platts_coal.csv', sep=';', index_col=0, encoding='latin-1')\n",
    "df_platts_2019 = pd.read_csv('.//tables//2019_df_platts_coal.csv', sep=';', index_col=0, encoding='latin-1')\n",
    "\n",
    "df_platts_abbrev = pd.read_csv('.//platts_2017_ABBREV.csv', sep=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the dataset where public finance data from the Global Coal Plant Finance Tracker from Global Energy Monitor\n",
    "#is manually linked to units\n",
    "\n",
    "folder = './/'\n",
    "filename = 'plants_finance_merge_Leon_GCPT_'+pz+'_GCFT_'+fz+'_deflation_20210224.csv' \n",
    "df_gem_gem = pd.read_csv(folder+filename ,sep=';', encoding='latin-1', decimal=',', thousands='.', index_col=0)\n",
    "\n",
    "# for df_gem_gem: sometimes values are nan, replace with 0:\n",
    "for x in ['Amount (in USD)','MW']:\n",
    "    df_gem_gem[x] = df_gem_gem[x].fillna(0)#.astype(int)\n",
    "\n",
    "#sometimes no Phase ID stated:\n",
    "n = 1\n",
    "for x in df_gem_gem.index:\n",
    "    if type(df_gem_gem.loc[x, 'Phase ID']) != str:\n",
    "        df_gem_gem.loc[x, 'Phase ID'] = 'PHx'+str(n)\n",
    "        n+=1\n",
    "        \n",
    "#include a column with the origin of the data:\n",
    "df_gem_gem['data'] = 'GCPFT_GEM'\n",
    "\n",
    "# add 'GCPFT_GEM' to Transaction Nr to differentiate from other dataset\n",
    "for x in df_gem_gem.index:\n",
    "    df_gem_gem.loc[x, 'Transaction Nr'] = str(df_gem_gem.loc[x, 'Transaction Nr'])+'_GCPFT_GEM'\n",
    "\n",
    "#rename a column, so it matches the column from urgewald\n",
    "df_gem_gem.rename(columns={'Status.plt':'Status'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the dataset where the mostly private finance data of the Global Coal Exit list from urgewald\n",
    "#is manually linked to unit\n",
    "folder = './/'\n",
    "filename = 'plants_finance_merge_Leon_GCPT_'+pz+'_urg_'+uz+'_deflation_unique_IDs_20210322.csv' \n",
    "df_urg_gem_unique_IDs = pd.read_csv(folder+filename ,sep=';', encoding='latin-1', decimal=',', thousands='.', index_col=0)\n",
    "\n",
    "# for df_urg_gem_unique_IDs: sometimes values are nan, replace with 0:\n",
    "for x in ['Amount (in USD 2020)', 'Per Investor Value (in mln US$)']:\n",
    "    df_urg_gem_unique_IDs[x] = df_urg_gem_unique_IDs[x].fillna(0)#.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the manually matched datasets, every transaction ID is only present once.\n",
    "# in the original dataset most IDs are stated multiple times, because of differing banks (syndicate loans)\n",
    "# match the individual unit-transactions with the whole dataset:\n",
    "df_urg_all = pd.read_csv('.//urgewald_Financing_of_GCEL_2020_creditor_nogreen+inscope+project+noIFI.csv' ,\\\n",
    "                     sep=';', encoding='latin-1', decimal=',', thousands='.')\n",
    "df_urg_gem = df_urg_gem_unique_IDs.copy()\n",
    "for x in df_urg_gem_unique_IDs.index:\n",
    "    for y in df_urg_all.index:\n",
    "        if ((df_urg_gem_unique_IDs.loc[x, 'Deal Number / Deal ID'] == df_urg_all.loc[y, 'Deal Number / Deal ID']) \\\n",
    "            and (df_urg_gem_unique_IDs.loc[x, 'Investor Parent'] != df_urg_all.loc[y, 'Investor Parent'])):\n",
    "            df_urg_gem = df_urg_gem.append(df_urg_gem_unique_IDs.loc[x], ignore_index=True)\n",
    "            var_last_index = df_urg_gem.index[-1]\n",
    "            df_urg_gem.loc[var_last_index, 'Investor Parent'] = df_urg_all.loc[y, 'Investor Parent']\n",
    "            df_urg_gem.loc[var_last_index, 'Investor Parent Country'] = df_urg_all.loc[y, 'Investor Parent Country']\n",
    "\n",
    "            df_urg_gem.loc[var_last_index, 'Per Investor Value (in mln US$)'] = df_urg_all.loc[y, 'Per Investor Value (in mln US$)']\n",
    "            df_urg_gem.loc[var_last_index, 'Amount (in USD 2020)'] = df_urg_gem.loc[var_last_index, 'Per Investor Value (in mln US$)']\\\n",
    "            *df_urg_gem.loc[var_last_index, 'Deflation_Factor_2020']*1000000\n",
    "            \n",
    "        #sometimes same deal number & bank, but different value!\n",
    "        elif ((df_urg_gem_unique_IDs.loc[x, 'Deal Number / Deal ID'] == df_urg_all.loc[y, 'Deal Number / Deal ID']) \\\n",
    "            and (df_urg_gem_unique_IDs.loc[x, 'Investor Parent'] == df_urg_all.loc[y, 'Investor Parent']) \\\n",
    "            and (df_urg_gem_unique_IDs.loc[x, 'Per Investor Value (in mln US$)'] != df_urg_all.loc[y, 'Per Investor Value (in mln US$)'])):\n",
    "            df_urg_gem = df_urg_gem.append(df_urg_gem_unique_IDs.loc[x], ignore_index=True)\n",
    "            var_last_index = df_urg_gem.index[-1]\n",
    "            df_urg_gem.loc[var_last_index, 'Investor Parent'] = df_urg_all.loc[y, 'Investor Parent']\n",
    "            df_urg_gem.loc[var_last_index, 'Investor Parent Country'] = df_urg_all.loc[y, 'Investor Parent Country']\n",
    "\n",
    "            df_urg_gem.loc[var_last_index, 'Per Investor Value (in mln US$)'] = df_urg_all.loc[y, 'Per Investor Value (in mln US$)']\n",
    "            df_urg_gem.loc[var_last_index, 'Amount (in USD 2020)'] = df_urg_gem.loc[var_last_index, 'Per Investor Value (in mln US$)']\\\n",
    "            *df_urg_gem.loc[var_last_index, 'Deflation_Factor_2020']*1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust df_urg_gem:\n",
    "\n",
    "#rename some columns so they match df_gem_gem:\n",
    "df_urg_gem.rename(columns={'Investor Parent':'Institution', 'Investor Parent Country':'Financing Country', \\\n",
    "        'Year.plt':'Year'}, inplace=True)\n",
    "\n",
    "#include a column with the origin of the data:\n",
    "df_urg_gem['data'] = 'GCEL_urg'\n",
    "\n",
    "#rename some columns that are used later so I can match the two financing datasets:\n",
    "df_urg_gem.rename(columns={'Investor Parent Country':'Financing Country',\\\n",
    "                           'Year.fin':'Year of Close', 'Deal Number / Deal ID':'Phase ID'}, inplace=True)\n",
    "\n",
    "# add 'GCEL_urg' to Transaction Nr to differentiate from other dataset\n",
    "for x in df_urg_gem.index:\n",
    "    df_urg_gem.loc[x, 'Transaction Nr'] = str(df_urg_gem.loc[x, 'Transaction Nr'])+'_GCEL_urg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two datasets\n",
    "df_urggem_gem = df_gem_gem.append(df_urg_gem, sort=False).reset_index(drop=True)\n",
    "\n",
    "#prepare dict with all three datasets\n",
    "dict_3_dfs = {'gem_GCFT':df_gem_gem, 'urg_GCEL':df_urg_gem, 'merged':df_urggem_gem}\n",
    "dict_dfs = {'gem_GCFT':df_gem_gem, 'urg_GCEL':df_urg_gem}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Get ownership of banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sheet where I stated the type of bank (state-owned / private / ECA) for each bank\n",
    "df_banks = pd.read_csv('.//banks//banks_grouped_20210727_nm.csv' ,sep=';', encoding='latin-1', decimal=',', thousands='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, check if all banks from df_urggem_gem are in df_banks:\n",
    "list_banks_not_yet_included =[]\n",
    "for x in df_urggem_gem.index:\n",
    "    list_banks = list(set(df_banks['other_name_1'])|set(df_banks['other_name_2'])|\\\n",
    "                      set(df_banks['other_name_3'])|set(df_banks['name']))\n",
    "    if df_urggem_gem.loc[x, 'Institution'] not in list_banks:\n",
    "        list_banks_not_yet_included.append(df_urggem_gem.loc[x, 'Institution'])\n",
    "        \n",
    "list_banks_not_yet_included = list(set(list_banks_not_yet_included))\n",
    "len(list_banks_not_yet_included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if banks are not yet included, match them manually and then import df again!\n",
    "df_banks_not_yet_included =  pd.DataFrame(data={'banks':list_banks_not_yet_included})\n",
    "df_banks_not_yet_included.to_csv('.//banks//banks_not_yet_included_20210322.csv' ,sep=';', encoding='latin-1', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of unique banks before = 232\n",
      "no of unique banks after = 215\n"
     ]
    }
   ],
   "source": [
    "# change name of banks, if there are different names for the same bank:\n",
    "print('no of unique banks before = '+ str(len(set(df_urggem_gem['Institution']))))\n",
    "for x in df_urggem_gem.index:\n",
    "    var_bank = df_urggem_gem.loc[x, 'Institution']\n",
    "    for y in ['other_name_1', 'other_name_2', 'other_name_3']:\n",
    "        for z in df_banks.index:\n",
    "            if var_bank == df_banks.loc[z, y]:\n",
    "                df_urggem_gem.loc[x, 'Institution'] = df_banks.loc[z, 'name']\n",
    "print('no of unique banks after = '+ str(len(set(df_urggem_gem['Institution']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include column with type of bank (state-owned / private / ECA)\n",
    "for x in df_urggem_gem.index:\n",
    "    var_bank = df_urggem_gem.loc[x, 'Institution']\n",
    "    df_urggem_gem.loc[x, 'ownership'] = df_banks[df_banks.name==var_bank]['ownership'].reset_index(drop=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 ECAs as Insurers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ECAs that provide exclusively credit insurance (not loans) are taken as a seperate column (not as banks). <br> The banks from urgewald are not considered as insurers, as they all proide either loans or underwriting of share and bond issuances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ECAs = list(set(df_urggem_gem[df_urggem_gem['ownership']=='ECA']['Institution']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df_urggem_gem.copy()\n",
    "df_no_ECA = df_a[~df_a.Institution.isin(list_ECAs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this ID will disappear: G100883\n",
      "this ID will disappear: G100884\n",
      "this ID will disappear: G100885\n",
      "this ID will disappear: G113103\n",
      "this ID will disappear: G108242\n",
      "this ID will disappear: G108241\n",
      "this ID will disappear: G100485\n",
      "this ID will disappear: G100484\n"
     ]
    }
   ],
   "source": [
    "# add insurance as own company in own column:\n",
    "# create df without the ECAs\n",
    "df_a = df_urggem_gem.copy()\n",
    "df_no_ECA = df_a[~df_a.Institution.isin(list_ECAs)]\n",
    "\n",
    "df_urggem_gem_ECAcol = df_urggem_gem.copy()\n",
    "for x in ['1','2']:\n",
    "    df_urggem_gem_ECAcol['Insurer_'+x] = ''\n",
    "    df_urggem_gem_ECAcol['Insurer_'+x+'_Country'] = ''\n",
    "\n",
    "for x in df_urggem_gem_ECAcol.index:\n",
    "    var_insurer = df_urggem_gem_ECAcol.loc[x, 'Institution']\n",
    "    var_ins_count = df_urggem_gem_ECAcol.loc[x, 'Financing Country']\n",
    "    if var_insurer in list_ECAs:\n",
    "        var_ID = df_urggem_gem_ECAcol.loc[x, 'TrackerID']\n",
    "        if var_ID in set(df_no_ECA['TrackerID']):\n",
    "            df_b = df_urggem_gem_ECAcol[df_urggem_gem_ECAcol['TrackerID']== var_ID]\n",
    "            for y in df_b.index:\n",
    "                if y in df_urggem_gem_ECAcol.index: #might be it is already dropped\n",
    "                    if df_urggem_gem_ECAcol.loc[y, 'Insurer_1'] == '':\n",
    "                        df_urggem_gem_ECAcol.loc[y, 'Insurer_1'] = var_insurer\n",
    "                        df_urggem_gem_ECAcol.loc[y, 'Insurer_1_Country'] = var_ins_count\n",
    "                    elif df_urggem_gem_ECAcol.loc[y, 'Insurer_1'] != var_insurer:\n",
    "                        df_urggem_gem_ECAcol.loc[y, 'Insurer_2'] = var_insurer\n",
    "                        df_urggem_gem_ECAcol.loc[y, 'Insurer_2_Country'] = var_ins_count\n",
    "            df_urggem_gem_ECAcol.drop(x, inplace=True)\n",
    "        else:\n",
    "            print('this ID will disappear: '+ var_ID) #To see, which units will be lost\n",
    "            df_urggem_gem_ECAcol.drop(x, inplace=True)\n",
    "df_urggem_gem_ECAcol.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Drop double countings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes one bank provides several loans for a unit. Merge these transactions. <br>\n",
    "This can be either between the two datasets or in either of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes to only get cross-border flows, i.e. fin country != site country\n",
    "def foreign_only(df):\n",
    "    for x in df.index:\n",
    "        if df.loc[x, 'Financing Country'] == df.loc[x, 'Country']:\n",
    "            df.drop(x, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of banks in GCPFT (state-owned & foreign only) by GEM = 43\n",
      "no of state-owned banks in urgewald with foreign fin = 11\n",
      "intersection of the two sets length = 5\n",
      "the intersecting banks are: {'Agricultural Bank of China (ABC)', 'Bank of China (BOC)', 'Bank of Communications', 'Industrial and Commercial Bank of China (ICBC)', 'China Construction Bank (CCB)'}\n",
      "the banks from urgewald that are not in the GEM data are: {'China Minsheng Banking', 'China Zheshang Bank', 'Anbang Insurance Group', 'Investment Corporation of Dubai', 'Abu Dhabi Investment Council', 'National Commercial Bank'}\n"
     ]
    }
   ],
   "source": [
    "# check, if the state-owned transactions from urgewald equal the ones from GEM\n",
    "df_a = df_urggem_gem_ECAcol.copy()\n",
    "df_a_gem = df_a[df_a['data']=='GCPFT_GEM']\n",
    "set_a_gem = set(df_a_gem['Institution'])\n",
    "df_a_urg = df_a[df_a['data']=='GCEL_urg']\n",
    "df_a_urg = df_a_urg[df_a_urg['ownership']=='state-owned']\n",
    "df_a_urg = foreign_only(df_a_urg)\n",
    "set_a_urg = set(df_a_urg['Institution'])\n",
    "\n",
    "print('no of banks in GCPFT (state-owned & foreign only) by GEM = '+str(len(set_a_gem)))\n",
    "print('no of state-owned banks in urgewald with foreign fin = '+str(len(set_a_urg)))\n",
    "set_b = set_a_gem & set_a_urg\n",
    "print('intersection of the two sets length = ' +str(len(set_b)))\n",
    "print('the intersecting banks are: '+ str(set_b))\n",
    "set_c = set_a_urg - set_a_gem\n",
    "print('the banks from urgewald that are not in the GEM data are: '+ str(set_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 banks in gem have 140 unit-transactions\n",
      "the 5 banks in urg have 138 unit-transactions\n",
      "the 5 banks in gem finance 83 units\n",
      "the 5 banks in urg finance 24 units\n",
      "intersection of the two sets length = 10\n"
     ]
    }
   ],
   "source": [
    "# check, for those 5 banks, do they have the same transactions (for the same units?)\n",
    "df_a_gem_5 = df_a_gem[df_a_gem['Institution'].isin(set_b)]\n",
    "df_a_urg_5 = df_a_urg[df_a_urg['Institution'].isin(set_b)]\n",
    "print('the 5 banks in gem have '+str(len(df_a_gem_5))+' unit-transactions')\n",
    "print('the 5 banks in urg have '+str(len(df_a_urg_5))+' unit-transactions')\n",
    "set_a_gem_5 = set(df_a_gem_5['TrackerID'])\n",
    "set_a_urg_5 = set(df_a_urg_5['TrackerID'])\n",
    "print('the 5 banks in gem finance '+str(len(set_a_gem_5))+' units')\n",
    "print('the 5 banks in urg finance '+str(len(set_a_urg_5))+' units')\n",
    "set_b_5 = set_a_gem_5 & set_a_urg_5\n",
    "print('intersection of the two sets length = ' +str(len(set_b_5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len before = 2813\n",
      "len before state-owned = 1462\n",
      "len before private = 1351\n",
      "\n",
      "len afterwards = 2527\n",
      "len afterwards state-owned = 1329\n",
      "len afterwards private = 1198\n"
     ]
    }
   ],
   "source": [
    "# sometimes one bank has several transactions for one unit -->merge\n",
    "# if the two (or more entries) are from the same source, add the transaction amounts, \\\n",
    "# if not, drop one (as it might be double counting)\n",
    "list_merged_IDs = []\n",
    "print('len before = '+str(len(df_urggem_gem_ECAcol)))\n",
    "print('len before state-owned = '+str(len(df_urggem_gem_ECAcol[df_urggem_gem_ECAcol['ownership']=='state-owned'])))\n",
    "print('len before private = '+str(len(df_urggem_gem_ECAcol[df_urggem_gem_ECAcol['ownership']=='private']))+'\\n')\n",
    "\n",
    "df_urggem_gem_ECAcol_nodoub = df_urggem_gem_ECAcol.copy()\n",
    "for x in df_urggem_gem_ECAcol_nodoub.index:\n",
    "    var_ID = df_urggem_gem_ECAcol_nodoub.loc[x, 'TrackerID']\n",
    "    var_deal = df_urggem_gem_ECAcol_nodoub.loc[x, 'Phase ID']\n",
    "    fin_a = df_urggem_gem_ECAcol_nodoub.loc[x, 'Institution']\n",
    "    var_data = df_urggem_gem_ECAcol_nodoub.loc[x, 'data']\n",
    "    for y in df_urggem_gem_ECAcol_nodoub.index:\n",
    "        if x in df_urggem_gem_ECAcol_nodoub.index:\n",
    "            if x !=y and df_urggem_gem_ECAcol_nodoub.loc[y, 'TrackerID'] == var_ID and \\\n",
    "            df_urggem_gem_ECAcol_nodoub.loc[y, 'Institution'] == fin_a and \\\n",
    "            df_urggem_gem_ECAcol_nodoub.loc[y, 'Phase ID'] == var_deal:\n",
    "                if df_urggem_gem_ECAcol_nodoub.loc[y, 'data'] == var_data:\n",
    "                    df_urggem_gem_ECAcol_nodoub.loc[y, 'Amount (in USD 2020)'] += \\\n",
    "                    df_urggem_gem_ECAcol_nodoub.loc[x, 'Amount (in USD 2020)']\n",
    "                    df_urggem_gem_ECAcol_nodoub.loc[y, 'Per Investor Value (in mln US$)'] += \\\n",
    "                    df_urggem_gem_ECAcol_nodoub.loc[x, 'Per Investor Value (in mln US$)']\n",
    "                list_merged_IDs.append(df_urggem_gem_ECAcol_nodoub.loc[y, 'TrackerID'])\n",
    "                df_urggem_gem_ECAcol_nodoub.drop(x, inplace=True)\n",
    "df_urggem_gem_ECAcol_nodoub.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('len afterwards = '+str(len(df_urggem_gem_ECAcol_nodoub)))\n",
    "print('len afterwards state-owned = '+str(len(df_urggem_gem_ECAcol_nodoub[df_urggem_gem_ECAcol_nodoub['ownership']=='state-owned'])))\n",
    "print('len afterwards private = '+str(len(df_urggem_gem_ECAcol_nodoub[df_urggem_gem_ECAcol_nodoub['ownership']=='private'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urggem_gem_ECAcol_nodoub.to_csv('.//tables//basis//df_urggem_gem_ECAcol_nodoub_GCPT_'+pz+'_GCFT_'+fz+'_urg_'+uz+'_20210728.csv', \\\n",
    "                       sep=';', encoding='latin-1', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 calculate weighted amounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example for transaction amount: a bank provided 10 mio in loans. We merged this transaction to two units, each of these unit-transactions would now have a transaction amount of 10mio. Thus we split it, i.e. 5mio for each unit-transaction. <br>\n",
    "example for capacity: if two banks provide finance for a plant with 500MW, the capacity is 500MW each, so double counting. Thus I include a column with the weighted capacity, i.e. 250MW each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'weighted_transaction_amount': Assigned each transaction-unit-link a weighted_transaction_amount (transaction amount divided by number of units matched to respective transaction)\n",
    "\n",
    "'mean_weighted_transaction_amount': Assigned each unit a mean_weighted_transaction_amount (total funding of one unit divided by the number of transactions it received). Only count those transactions that are not 0 to calculate the mean. \n",
    "\n",
    "'total_funding_amount_estimated': Assigned each unit a total_funding_amount_estimated (sum of weighted_transaction_amounts of all transactions to respective unit plus mean_weighted_transaction_amount for each transaction to respective unit for which amount information was not found). Attention: total_funding_amount_estimated of transactions linked to multiple units was counted multiple times\n",
    "\n",
    "'weighted_funding_amount_estimated': per unit-transaction: Assigned each transaction-unit-link a weighted_funding_amount_estimated.\n",
    "weighted_funding_amount_estimated is - if not 0 - the weighted_transaction_amount, otherwise the mean_weighted_transaction_amount for the respective unit.\n",
    "\n",
    "'funding_share': per unit-transaction: calculate the share of each unit-transaction for a unit. The total sum should equal the no of unique units in the dataframe.\n",
    "\n",
    "'capacity_weighted_by_funding_share'  Calculate share of capacity (Capacity (MW)) equal to share of finance amount. This makes the actual capacity as of manual matching that was funded/built receiving the transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urggem_gem_ECAcol_nodoub_wei = df_urggem_gem_ECAcol_nodoub.copy()\n",
    "\n",
    "# Create columns\n",
    "for x in ['weighted_transaction_amount_mio', 'mean_weighted_transaction_amount_mio', 'total_funding_amount_estimated_mio', \\\n",
    "         'weighted_funding_amount_estimated_mio', 'funding_share', 'capacity_weighted_by_funding_share_MW']:\n",
    "    df_urggem_gem_ECAcol_nodoub_wei[x] = 0\n",
    "\n",
    "# sometimes nan, replace with 0\n",
    "for x in ['Amount (in USD 2020)','Capacity (MW)']:\n",
    "    df_urggem_gem_ECAcol_nodoub_wei[x] = df_urggem_gem_ECAcol_nodoub_wei[x].fillna(0)\n",
    "df_urggem_gem_ECAcol_nodoub_wei['Amount (in USD 2020)_mio'] = df_urggem_gem_ECAcol_nodoub_wei['Amount (in USD 2020)']/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'weighted_transaction_amount'\n",
    "for x in df_urggem_gem_ECAcol_nodoub_wei.index:\n",
    "    var_phaseID = df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'Phase ID']\n",
    "    df_a = df_urggem_gem_ECAcol_nodoub_wei[df_urggem_gem_ECAcol_nodoub_wei['Phase ID']==var_phaseID]\n",
    "    var_no_of_units = len(set(df_a['TrackerID']))\n",
    "    var_amount = df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'Amount (in USD 2020)_mio']\n",
    "    if var_amount == 0:\n",
    "        df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'weighted_transaction_amount_mio'] = 0\n",
    "    else:\n",
    "        df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'weighted_transaction_amount_mio'] = \\\n",
    "        var_amount/var_no_of_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'mean_weighted_transaction_amount'\n",
    "for x in df_urggem_gem_ECAcol_nodoub_wei.index:\n",
    "    var_TrackerID = df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'TrackerID']\n",
    "    df_a = df_urggem_gem_ECAcol_nodoub_wei.copy()\n",
    "    df_b = df_a[df_a['TrackerID']==var_TrackerID]\n",
    "    var_amount = df_b['weighted_transaction_amount_mio'].sum()\n",
    "    df_c = df_b[df_b['weighted_transaction_amount_mio']!=0]\n",
    "    var_no_of_trans_no_0 = len(df_c)\n",
    "    if var_amount == 0:\n",
    "        df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'mean_weighted_transaction_amount_mio'] = 0\n",
    "    else:\n",
    "        var_mean = var_amount/var_no_of_trans_no_0\n",
    "        df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'mean_weighted_transaction_amount_mio'] = var_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'total_funding_amount_estimated'\n",
    "for x in df_urggem_gem_ECAcol_nodoub_wei.index:  \n",
    "    var_TrackerID = df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'TrackerID']\n",
    "    df_a = df_urggem_gem_ECAcol_nodoub_wei.copy()\n",
    "    df_b = df_a[df_a['TrackerID']==var_TrackerID].reset_index(drop=True)\n",
    "    var_total = 0\n",
    "    for y in df_b.index:\n",
    "        if df_b.loc[y, 'weighted_transaction_amount_mio'] == 0:\n",
    "            var_total += df_b.loc[y, 'mean_weighted_transaction_amount_mio']\n",
    "        else:\n",
    "            var_total += df_b.loc[y, 'weighted_transaction_amount_mio']\n",
    "    df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'total_funding_amount_estimated_mio'] = var_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'weighted_funding_amount_estimated'\n",
    "for x in df_urggem_gem_ECAcol_nodoub_wei.index:  \n",
    "    if df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'weighted_transaction_amount_mio'] == 0:\n",
    "        df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'weighted_funding_amount_estimated_mio'] = \\\n",
    "        df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'mean_weighted_transaction_amount_mio']\n",
    "    else:\n",
    "        df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'weighted_funding_amount_estimated_mio'] = \\\n",
    "        df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'weighted_transaction_amount_mio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'funding_share'\n",
    "#P: for some units the 'total_funding_amount_estimated' and the 'weighted_funding_amount_estimated' are 0, thus share aso 0.\n",
    "# in that case split share among the transactions for a unit.\n",
    "list_IDs_0 = \\\n",
    "set(df_urggem_gem_ECAcol_nodoub_wei[df_urggem_gem_ECAcol_nodoub_wei['total_funding_amount_estimated_mio']==0]['TrackerID'])\n",
    "\n",
    "for x in df_urggem_gem_ECAcol_nodoub_wei.index: \n",
    "    var_ID = df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'TrackerID']\n",
    "    if var_ID in list_IDs_0:\n",
    "        df_a = df_urggem_gem_ECAcol_nodoub_wei[df_urggem_gem_ECAcol_nodoub_wei['TrackerID'] == var_ID]\n",
    "        var_no_units = len(df_a)\n",
    "        var_share = 1/var_no_units\n",
    "        df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'funding_share'] = var_share\n",
    "    else:\n",
    "        df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'funding_share'] = \\\n",
    "        df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'weighted_funding_amount_estimated_mio']/\\\n",
    "        df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'total_funding_amount_estimated_mio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'capacity_weighted_by_funding_share'\n",
    "for x in df_urggem_gem_ECAcol_nodoub_wei.index:\n",
    "    var_share = df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'funding_share']\n",
    "    var_capa = df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'Capacity (MW)']\n",
    "    var_capa_weigh = var_share*var_capa\n",
    "    if var_capa == 0:\n",
    "        df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'capacity_weighted_by_funding_share_MW'] = 0\n",
    "    else:\n",
    "        df_urggem_gem_ECAcol_nodoub_wei.loc[x, 'capacity_weighted_by_funding_share_MW'] = var_capa_weigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount (in USD 2020)_mio sum = 334343.6982629417\n",
      "weighted_transaction_amount_mio sum = 141026.45288908598\n",
      "mean_weighted_transaction_amount_mio sum = 163033.7162501834\n",
      "total_funding_amount_estimated_mio sum = 2118270.767046257\n",
      "weighted_funding_amount_estimated_mio sum = 163033.7162501834\n",
      "funding_share sum = 459.0\n",
      "capacity_weighted_by_funding_share_MW sum = 225484.09999999998\n",
      "number of unique units (should equal sum of funding share) is 459\n",
      "capa of uniue units (should equal sum of capacit_weighted) is 225484.1\n"
     ]
    }
   ],
   "source": [
    "#Check the vaules for each new column\n",
    "for x in ['Amount (in USD 2020)_mio', 'weighted_transaction_amount_mio', 'mean_weighted_transaction_amount_mio', \\\n",
    "          'total_funding_amount_estimated_mio', 'weighted_funding_amount_estimated_mio', \\\n",
    "          'funding_share', 'capacity_weighted_by_funding_share_MW']:\n",
    "    print(x + ' sum = ' + str(df_urggem_gem_ECAcol_nodoub_wei[x].sum()))\n",
    "print('number of unique units (should equal sum of funding share) is '+ \\\n",
    "      str(len(set(df_urggem_gem_ECAcol_nodoub_wei['TrackerID']))))\n",
    "df_a = df_urggem_gem_ECAcol_nodoub_wei.copy()\n",
    "var_unique_sum = df_a.drop_duplicates(subset = ['TrackerID'])['Capacity (MW)'].sum()\n",
    "print('capa of uniue units (should equal sum of capacit_weighted) is '+ str(var_unique_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urggem_gem_ECAcol_nodoub_wei.to_csv('.//tables//basis//df_urggem_gem_ECAcol_nodoub_wei_GCPT_'+pz+'_GCFT_'+fz+'_urg_'+uz+'_20210728.csv', \\\n",
    "                       sep=';', encoding='latin-1', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6 Provide overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urggem_gem_ECAcol_nodoub_wei = pd.read_csv('.//tables//basis//df_urggem_gem_ECAcol_nodoub_wei_GCPT_'+pz+'_GCFT_'+fz+'_urg_'+uz+'_20210728.csv', \\\n",
    "                       sep=';', encoding='latin-1', decimal=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat dicts\n",
    "df_both = df_urggem_gem_ECAcol_nodoub_wei.copy()\n",
    "df_state = df_both[df_both['ownership']=='state-owned'].reset_index(drop=True)\n",
    "df_private = df_both[df_both['ownership']=='private'].reset_index(drop=True)\n",
    "\n",
    "dict_3_dfs = {'both':df_urggem_gem_ECAcol_nodoub_wei, 'state_owned':df_state, 'private':df_private}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for both\n",
      "DOUBLE COUNTING! from both: total amount in USD mio = 334343.69826293935\n",
      "from both: weighted total amount in USD mio = 141026.45288908592\n",
      "DOUBLE COUNTING! from both (all matched units): total MW = 1587863.1999999997\n",
      "from both & GCPT: weighted (matched units): MW by share = 225484.1\n",
      "number of unique transactions = 429\n",
      "number of distinct units = 459\n",
      "number of distinct plants = 204\n",
      "number of unit-transactions = 2527\n",
      "\n",
      "for state_owned\n",
      "DOUBLE COUNTING! from state_owned: total amount in USD mio = 259340.43896294152\n",
      "from state_owned: weighted total amount in USD mio = 104489.62388908562\n",
      "DOUBLE COUNTING! from state_owned (all matched units): total MW = 763132.1999999997\n",
      "from state_owned & GCPT: weighted (matched units): MW by share = 194601.97665384648\n",
      "number of unique transactions = 367\n",
      "number of distinct units = 440\n",
      "number of distinct plants = 191\n",
      "number of unit-transactions = 1329\n",
      "\n",
      "for private\n",
      "DOUBLE COUNTING! from private: total amount in USD mio = 75003.25929999986\n",
      "from private: weighted total amount in USD mio = 36536.82899999997\n",
      "DOUBLE COUNTING! from private (all matched units): total MW = 824731.0\n",
      "from private & GCPT: weighted (matched units): MW by share = 30882.123346153323\n",
      "number of unique transactions = 105\n",
      "number of distinct units = 98\n",
      "number of distinct plants = 46\n",
      "number of unit-transactions = 1198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for abc in dict_3_dfs:\n",
    "    df_abc = dict_3_dfs[abc]\n",
    "    print('for '+ abc)\n",
    "    print('DOUBLE COUNTING! from '+ abc +': total amount in USD mio = ' + str(sum(df_abc['Amount (in USD 2020)_mio'])))\n",
    "    print('from '+ abc +': weighted total amount in USD mio = ' + str(sum(df_abc['weighted_transaction_amount_mio'])))\n",
    "    print('DOUBLE COUNTING! from '+ abc +' (all matched units): total MW = ' + str(sum(df_abc['Capacity (MW)'])))\n",
    "    print('from '+ abc +' & GCPT: weighted (matched units): MW by share = ' + str(sum(df_abc['capacity_weighted_by_funding_share_MW'])))\n",
    "    print('number of unique transactions = ' + str(len(set(df_abc['Transaction Nr']))))\n",
    "    print('number of distinct units = ' + str(len(set(df_abc['TrackerID']))))\n",
    "    print('number of distinct plants = ' + str(len(set(df_abc['TrackerLOC']))))\n",
    "    print('number of unit-transactions = ' + str(len(df_abc)) +'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Prepare final table for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create table with all unique units from both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 'long' version with all columns\n",
    "df_unique_units_long = df_both.copy()\n",
    "df_unique_units_long = df_unique_units_long.drop_duplicates(subset = ['TrackerID']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a version with few columns only\n",
    "df_unique_units = df_unique_units_long[['TrackerID', 'TrackerLOC', 'Unit', 'Plant', \\\n",
    "                                   'Country','Sponsor', 'Parent', 'Capacity (MW)','Year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Merge with Platts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Check, if unit already manually matched with Platts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_units_already_matched = \\\n",
    "    pd.read_csv('.//tables//df_units_matched_GCPT_GCFT_urg_Platts19_20210728_nm.csv' ,\\\n",
    "                                      sep=';', encoding='latin-1', decimal=',', index_col=0)\n",
    "df_units_already_matched.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 not yet matched with platts\n"
     ]
    }
   ],
   "source": [
    "# check for each unit, if already in the df_units_already_matched. If not, add!\n",
    "a = 0\n",
    "for x in df_unique_units.index:\n",
    "    var_ID = df_unique_units.loc[x, 'TrackerID']\n",
    "    if var_ID not in df_units_already_matched['TrackerID'].values:\n",
    "        a += 1\n",
    "        var_len = len(df_units_already_matched)\n",
    "        for y in ['TrackerID', 'TrackerLOC', 'Unit', 'Plant', 'Country', 'Sponsor','Parent', 'Capacity (MW)', 'Year']:\n",
    "            df_units_already_matched.loc[var_len, y] = df_unique_units.loc[x, y]\n",
    "print( str(a) + ' not yet matched with platts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_units_already_matched.to_csv('.//tables//df_units_matched_GCPT_GCFT_urg_Platts_20210323.csv', \\\n",
    "                       sep=';', encoding='latin-1', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Get information on companies from Platts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import table with unique units matched with Platts\n",
    "df_all_unique_units_matched = \\\n",
    "    pd.read_csv('.//tables//df_units_matched_GCPT_GCFT_urg_Platts19_20210728_nm.csv' ,\\\n",
    "                                      sep=';', encoding='latin-1', decimal=',', index_col=0) #20210323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TURBMFR Turbine manufacturer or IC engine manufacturer; \n",
    "#COMPANY Operator or primary owner of generating unit. \n",
    "#SSSMFR Steam supply system (boiler or reactor) manufacturer, N/A for other technologies. \n",
    "#GENMFR Generator manufacturer. \n",
    "#AE Primary architect/engineering firm. \n",
    "#CONSTRUCT Primary construction contractor. \n",
    "#PARENT Parent company of plant operator, generally ultimate parent, not immediate parent. \\\n",
    "    #For sorting purposes, the Company has been repeated if there is no Parent identified.\n",
    "    \n",
    "#not relevant:\n",
    "    #PARTMFR Manufacturer of particulate control device for unit. May be N/A. \n",
    "    #FGDMFR Manufacturer of FGD scrubber system, if applicable. May be N/A. \n",
    "    #NOXMFR Manufacturer of NOX control system, if applicable. May be N/A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_company_types_from_Platts = ['TURBMFR', 'GENMFR', 'PARTMFR', 'FGDMFR', 'NOXMFR', 'SSSMFR',  'AE', 'CONSTRUCT' ]\n",
    "list_company_types_all = ['Sponsor', 'Parent', \\\n",
    "        'TURBMFR', 'GENMFR', 'PARTMFR', 'FGDMFR', 'NOXMFR', 'SSSMFR',  'AE', 'CONSTRUCT' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get companies from platts, check if either in 2017 or 2019\n",
    "for x in df_all_unique_units_matched.index:\n",
    "    if not np.isnan(df_all_unique_units_matched.loc[x, 'UNITID Platts']):\n",
    "        var_ID = df_all_unique_units_matched.loc[x, 'UNITID Platts']\n",
    "        if var_ID in list(df_platts_2019['UNITID']):\n",
    "            for y in df_platts_2019.index:\n",
    "                if var_ID == df_platts_2019.loc[y, 'UNITID']:\n",
    "                    for z in list_company_types_from_Platts:\n",
    "                        df_all_unique_units_matched.loc[x, z] = df_platts_2019.loc[y, z]\n",
    "        else:\n",
    "            for y in df_platts_2017.index:\n",
    "                if var_ID == df_platts_2017.loc[y, 'UNITID']:\n",
    "                    for z in list_company_types_from_Platts:\n",
    "                        df_all_unique_units_matched.loc[x, z] = df_platts_2017.loc[y, z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if only parent is stated, not sponsor: take sponsor as parent as well:\n",
    "for x in df_all_unique_units_matched.index:\n",
    "    if type(df_all_unique_units_matched.loc[x, 'Parent']) != str:\n",
    "        if type(df_all_unique_units_matched.loc[x, 'Sponsor']) == str:\n",
    "            df_all_unique_units_matched.loc[x, 'Parent'] = df_all_unique_units_matched.loc[x, 'Sponsor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrackerID              491\n",
       "TrackerLOC             491\n",
       "Unit                   491\n",
       "Plant                  491\n",
       "Country                491\n",
       "Sponsor                491\n",
       "Parent                 491\n",
       "Capacity (MW)          491\n",
       "Year                   478\n",
       "UNIT name Platts       443\n",
       "PLANT name Platts      443\n",
       "UNITID Platts          443\n",
       "Matching Confidence    491\n",
       "comment                  4\n",
       "TURBMFR                210\n",
       "GENMFR                 207\n",
       "PARTMFR                 45\n",
       "FGDMFR                  38\n",
       "NOXMFR                  22\n",
       "SSSMFR                 202\n",
       "AE                     260\n",
       "CONSTRUCT              259\n",
       "dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how many are not nan / empty\n",
    "df_all_unique_units_matched.notna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Add company information from GCFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check: Do some units not have information on Platts but in the merged df from the GCFT?\n",
    "# of course this only works for the plants from GEM GCFT, not urgewald!\n",
    "# by 'TrackerID' (from GCPT)\n",
    "# Platts (so df_all_unique_units_matched) : \n",
    "list_tpes_platts = ['TURBMFR', 'GENMFR', 'SSSMFR', 'CONSTRUCT']\n",
    "# GCFT (from the 2019 version. In 2020 no construction and other names!) (so df_unique_units_long) : no 'AE'\n",
    "list_types_GCFT = ['TurbMFR', 'Equipment (GENMFR)','SSSMFR','Construction']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_comp_types = dict(zip(list_tpes_platts, list_types_GCFT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if units have no information from Platts, but from GCFT, add these info\n",
    "a = 0\n",
    "for x in df_all_unique_units_matched.index:\n",
    "    for y in df_unique_units_long.index:\n",
    "        if df_all_unique_units_matched.loc[x, 'TrackerID'] == df_unique_units_long.loc[y, 'TrackerID']:\n",
    "            for z in dict_comp_types:\n",
    "                if type(df_all_unique_units_matched.loc[x, z]) != str:\n",
    "                    if df_unique_units_long.loc[y, dict_comp_types[z]] != 'TBD':\n",
    "                        df_all_unique_units_matched.loc[x, z] = df_unique_units_long.loc[y, dict_comp_types[z]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrackerID              491\n",
       "TrackerLOC             491\n",
       "Unit                   491\n",
       "Plant                  491\n",
       "Country                491\n",
       "Sponsor                491\n",
       "Parent                 491\n",
       "Capacity (MW)          491\n",
       "Year                   478\n",
       "UNIT name Platts       443\n",
       "PLANT name Platts      443\n",
       "UNITID Platts          443\n",
       "Matching Confidence    491\n",
       "comment                  4\n",
       "TURBMFR                224\n",
       "GENMFR                 216\n",
       "PARTMFR                 45\n",
       "FGDMFR                  38\n",
       "NOXMFR                  22\n",
       "SSSMFR                 204\n",
       "AE                     260\n",
       "CONSTRUCT              276\n",
       "dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check again, if now more information on companies\n",
    "df_all_unique_units_matched.notna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Split companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "often multiple companies are stated -->split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns in df: should be with '_1'\n",
    "for x in list_company_types_all:\n",
    "    df_all_unique_units_matched[x+'_1'] = df_all_unique_units_matched[x]\n",
    "    df_all_unique_units_matched.drop(columns=x, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat list with the numbers \n",
    "list_company_types_all_no = []\n",
    "for x in list_company_types_all:\n",
    "    list_company_types_all_no.append(x+'_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not split =101\n",
      "split by ',' = 258\n",
      "split by ';' = 5\n"
     ]
    }
   ],
   "source": [
    "# sometimes split with \"/\" (Platts), \",\" (GCFT, GCPT), \";\" (just in case!)\n",
    "\n",
    "n = 0\n",
    "m = 0\n",
    "o = 0\n",
    "\n",
    "for x in df_all_unique_units_matched.index:\n",
    "    for y in list_company_types_all:\n",
    "        var_unsplit_name = df_all_unique_units_matched.loc[x, y+'_1']\n",
    "        if type(var_unsplit_name)==str:\n",
    "            if len(var_unsplit_name.split('/')) >1:\n",
    "                for z in range(0,len(var_unsplit_name.split('/'))):\n",
    "                    df_all_unique_units_matched.loc[x, y+'_'+str(z+1)] = var_unsplit_name.split('/')[z].strip()\n",
    "                    if y+'_'+str(z+1) not in list_company_types_all_no:\n",
    "                        list_company_types_all_no.append(y+'_'+str(z+1))\n",
    "                n+=1\n",
    "            elif len(var_unsplit_name.split(',')) >1:\n",
    "                for z in range(0,len(var_unsplit_name.split(','))):\n",
    "                    df_all_unique_units_matched.loc[x, y+'_'+str(z+1)] = var_unsplit_name.split(',')[z].strip()\n",
    "                    if y+'_'+str(z+1) not in list_company_types_all_no:\n",
    "                        list_company_types_all_no.append(y+'_'+str(z+1))\n",
    "                m+=1\n",
    "            elif len(var_unsplit_name.split(';')) >1:\n",
    "                for z in range(0,len(var_unsplit_name.split(';'))):\n",
    "                    df_all_unique_units_matched.loc[x, y+'_'+str(z+1)] = var_unsplit_name.split(';')[z].strip()\n",
    "                    if y+'_'+str(z+1) not in list_company_types_all_no:\n",
    "                        list_company_types_all_no.append(y+'_'+str(z+1))\n",
    "                o+=1\n",
    "print(\"split by '/' = \" +str(n))\n",
    "print(\"split by ',' = \" +str(m))\n",
    "print(\"split by ';' = \" +str(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 times company is just ' ', sometimes '0', or 'TBD' -->replace\n",
    "for x in df_all_unique_units_matched.index:\n",
    "    for y in list_company_types_all_no:\n",
    "        list_to_replace=['', '0', 'TBD']\n",
    "        for z in list_to_replace:\n",
    "            if df_all_unique_units_matched.loc[x,y] == z:\n",
    "                df_all_unique_units_matched.loc[x,y] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if only parent is stated, not sponsor: take sponsor as parent as well: (do it again)\n",
    "for x in df_all_unique_units_matched.index:\n",
    "    if type(df_all_unique_units_matched.loc[x, 'Parent_1']) != str:\n",
    "        if type(df_all_unique_units_matched.loc[x, 'Sponsor_1']) == str:\n",
    "            df_all_unique_units_matched.loc[x, 'Parent_1'] = df_all_unique_units_matched.loc[x, 'Sponsor_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "655"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make list with all split company names\n",
    "list_all_split_companies = []\n",
    "for x in list_company_types_all_no:\n",
    "    for y in df_all_unique_units_matched.index:\n",
    "        if type(df_all_unique_units_matched.loc[y, x]) == str:\n",
    "            list_all_split_companies.append(df_all_unique_units_matched.loc[y, x])\n",
    "list_all_split_companies = list(set(list_all_split_companies))\n",
    "len(list_all_split_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_unique_units_matched.to_csv('.//tables//df_units_matched_GCPT_GCFT_urg_Platts_split_20210728.csv', \\\n",
    "                       sep=';', encoding='latin-1', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Get countries of companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the countries provided in the datasets and complemented them by manually searching for the companies' countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import current version with company names with automatically & manually added countries\n",
    "df_split_abbrev_and_full_names_countries_nm = \\\n",
    "    pd.read_csv('.//tables//df_platts_GCFT_urg_company_split_names_countries_20210324_nm.csv',decimal=',', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list with all company names that are currently in the df:\n",
    "list_columns_with_company_names_old = ['ABBREV', 'other_name_1', 'other_name_2', 'other_name_3', 'other_name_4',\n",
    "       'other_name_5', 'FULLNAME', 'name_clean']\n",
    "list_company_names_old = []\n",
    "for x in list_columns_with_company_names_old:\n",
    "    list_company_names_old.extend(df_split_abbrev_and_full_names_countries_nm[x].values)\n",
    "list_company_names_old = list(set(list_company_names_old))\n",
    "len(list_company_names_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check, if there are new companies that are not yet in the list\n",
    "# print new names and add to list\n",
    "list_company_names_only_new_ones = []\n",
    "for x in df_all_unique_units_matched.index:\n",
    "    for y in list_company_types_all_no:\n",
    "        var_comp_name = df_all_unique_units_matched.loc[x,y]\n",
    "        if type(var_comp_name) == str:\n",
    "            if var_comp_name not in list_company_names_old:\n",
    "                list_company_names_only_new_ones.append(var_comp_name)\n",
    "list_company_names_only_new_ones = list(set(list_company_names_only_new_ones))\n",
    "len(list_company_names_only_new_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new names from list to df as ABBREV\n",
    "for x in list_company_names_only_new_ones:\n",
    "    new_row = {'ABBREV':x}\n",
    "    df_split_abbrev_and_full_names_countries_nm = df_split_abbrev_and_full_names_countries_nm.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbrev_to_full_name(abbrev):\n",
    "    a = abbrev\n",
    "    for x in df_platts_abbrev.index:\n",
    "        if df_platts_abbrev.loc[x, 'ABBREV'] == abbrev:\n",
    "            a = df_platts_abbrev.loc[x, 'FULLNAME']\n",
    "            break\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if no fullname given yet, add it. In Platts the country is sometimes stated as well. If so, take it!\n",
    "for x in df_split_abbrev_and_full_names_countries_nm.index:\n",
    "    if type(df_split_abbrev_and_full_names_countries_nm.loc[x,'FULLNAME']) != str:\n",
    "        var_abbrev = df_split_abbrev_and_full_names_countries_nm.loc[x,'ABBREV']\n",
    "        list_split = abbrev_to_full_name(var_abbrev).split(\"[\")\n",
    "        if len(list_split) !=1:\n",
    "            df_split_abbrev_and_full_names_countries_nm.loc[x,'FULLNAME'] = abbrev_to_full_name(var_abbrev)\n",
    "            df_split_abbrev_and_full_names_countries_nm.loc[x,'COUNTRY'] = list_split[1].replace(\"]\", \"\").replace(\")\", \"\")\n",
    "        else:\n",
    "            df_split_abbrev_and_full_names_countries_nm.loc[x,'FULLNAME'] = abbrev_to_full_name(var_abbrev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check in table from MA where I manually added countries of developers!\n",
    "df_dev_HQ_from_MA = pd.read_csv('.//tables//dev_HQ.csv', decimal=',', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_split_abbrev_and_full_names_countries_nm.index:\n",
    "    if type(df_split_abbrev_and_full_names_countries_nm.loc[x, 'COUNTRY']) != str:\n",
    "        for y in df_dev_HQ_from_MA.index:\n",
    "            if df_split_abbrev_and_full_names_countries_nm.loc[x, 'ABBREV'] == df_dev_HQ_from_MA.loc[y, 'DEV']:\n",
    "                df_split_abbrev_and_full_names_countries_nm.loc[x, 'COUNTRY'] = df_dev_HQ_from_MA.loc[y, 'DEV Country']\n",
    "            elif df_split_abbrev_and_full_names_countries_nm.loc[x, 'FULLNAME'] == df_dev_HQ_from_MA.loc[y, 'DEV']:\n",
    "                df_split_abbrev_and_full_names_countries_nm.loc[x, 'COUNTRY'] = df_dev_HQ_from_MA.loc[y, 'DEV Country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export this df so I can add the countries manually:\n",
    "df_split_abbrev_and_full_names_countries_nm.to_csv('.//tables//df_platts_GCFT_urg_company_split_names_countries_20210729.csv', \\\n",
    "    decimal=',', sep=';') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.6 Add countries to dataset with unique units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_unique_units_matched = pd.read_csv('.//tables//df_units_matched_GCPT_GCFT_urg_Platts_split_20210728.csv', \\\n",
    "                       sep=';', encoding='latin-1', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import version with manually added countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import current version with automatically & manually added countries\n",
    "df_split_abbrev_and_full_names_countries_nm = \\\n",
    "    pd.read_csv('.//tables//df_platts_GCFT_urg_company_split_names_countries_20210729_nm.csv',decimal=',', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "637"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check: how many unique companies (by clean name)?\n",
    "len(set(df_split_abbrev_and_full_names_countries_nm['name_clean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get countries\n",
    "df_all_unique_units_matched_countries = df_all_unique_units_matched.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change all company names to df_split_abbrev_and_full_names_countries_nm['name_clean']:\n",
    "for x in df_all_unique_units_matched_countries.index:\n",
    "    for y in list_company_types_all_no:\n",
    "        if type(df_all_unique_units_matched_countries.loc[x,y])==str:\n",
    "            for z in df_split_abbrev_and_full_names_countries_nm.index:\n",
    "                for xyz in ['ABBREV', 'other_name_1', 'other_name_2', 'other_name_3', 'other_name_4', 'other_name_5', 'FULLNAME']:\n",
    "                    if df_all_unique_units_matched_countries.loc[x,y] == df_split_abbrev_and_full_names_countries_nm.loc[z,xyz]:\n",
    "                        df_all_unique_units_matched_countries.loc[x,y] = df_split_abbrev_and_full_names_countries_nm.loc[z,'name_clean']\n",
    "                        df_all_unique_units_matched_countries.loc[x,y+'_Country'] = df_split_abbrev_and_full_names_countries_nm.loc[z,'COUNTRY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_unique_units_matched_countries.to_csv\\\n",
    "    ('.//tables//df_unique_units_countries_plants_finance_merge_GCPT_'+pz+'_GCFT_'+fz+'_urg_'+uz+'_platts_'+tz+'_20210729.csv' ,\\\n",
    "                                      sep=';', encoding='latin-1', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Finalize and export dataset with unique units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_unique_units_matched_countries = pd.read_csv\\\n",
    "    ('.//tables//df_unique_units_countries_plants_finance_merge_GCPT_'+pz+'_GCFT_'+fz+'_urg_'+uz+'_platts_'+tz+'_20210729.csv' ,\\\n",
    "                                      sep=';', encoding='latin-1', decimal=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some IDs in df_all_unique_units_matched_countries_short are from older versions. Drop:\n",
    "df_only_matched_units = df_all_unique_units_matched_countries.copy()\n",
    "\n",
    "for x in df_only_matched_units.index:\n",
    "    if df_only_matched_units.loc[x, 'TrackerID'] not in df_both['TrackerID'].values:\n",
    "        df_only_matched_units.drop(x, inplace=True)\n",
    "len(df_only_matched_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the order of columns\n",
    "df_only_matched_units_order = df_only_matched_units[['TrackerID', 'TrackerLOC', 'Unit', 'Plant', 'Country', 'Capacity (MW)',\\\n",
    "       'Year', 'UNIT name Platts', 'PLANT name Platts', 'UNITID Platts',\\\n",
    "       'Matching Confidence', 'Sponsor_1', 'Sponsor_1_Country', \\\n",
    "        'Sponsor_2', 'Sponsor_2_Country', 'Sponsor_3', 'Sponsor_3_Country', 'Sponsor_4', 'Sponsor_4_Country', \\\n",
    "        'Parent_1', 'Parent_1_Country', 'Parent_2', 'Parent_2_Country', 'Parent_3', 'Parent_3_Country', \\\n",
    "        'Parent_4', 'Parent_4_Country', 'Parent_5', 'Parent_5_Country', 'Parent_6', 'Parent_6_Country', \\\n",
    "        'TURBMFR_1', 'TURBMFR_1_Country', 'TURBMFR_2', 'TURBMFR_2_Country', 'GENMFR_1', 'GENMFR_1_Country', \\\n",
    "        'GENMFR_2', 'GENMFR_2_Country', 'PARTMFR_1','PARTMFR_1_Country', \\\n",
    "       'FGDMFR_1', 'FGDMFR_1_Country', 'NOXMFR_1', 'NOXMFR_1_Country', 'SSSMFR_1', 'SSSMFR_1_Country', \\\n",
    "        'AE_1',  'AE_1_Country', 'AE_2', 'AE_2_Country', 'AE_3', 'AE_3_Country', 'CONSTRUCT_1','CONSTRUCT_1_Country', \\\n",
    "        'CONSTRUCT_2', 'CONSTRUCT_2_Country', 'CONSTRUCT_3',  'CONSTRUCT_3_Country', 'CONSTRUCT_4',  'CONSTRUCT_4_Country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    " #    print(df_only_matched_units_order.notna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_matched_units_order.to_csv\\\n",
    "    ('.//tables//df_only_matched_units_order_countries_plants_finance_merge_GCPT_'+pz+'_GCFT_'+fz+'_urg_'+uz+'_platts_'+tz+'_20210729.csv' ,\\\n",
    "                                      sep=';', encoding='latin-1', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Add companies and countries to the merged unit-transaction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both = df_urggem_gem_ECAcol_nodoub_wei.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_matched_units_order = pd.read_csv\\\n",
    "    ('.//tables//df_only_matched_units_order_countries_plants_finance_merge_GCPT_'+pz+'_GCFT_'+fz+'_urg_'+uz+'_platts_'+tz+'_20210729.csv' ,\\\n",
    "                                      sep=';', encoding='latin-1', decimal=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some columns that are already in the merged unit-transaction df\n",
    "df_only_matched_units_order_short = df_only_matched_units_order.copy()\n",
    "list_drop_a = ['TrackerLOC', 'Unit', 'Plant', 'Country', 'Capacity (MW)', 'Year']\n",
    "\n",
    "for x in list_drop_a:\n",
    "    df_only_matched_units_order_short.drop(columns=x, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some columns in df_both before merging the two:\n",
    "list_drop_b = ['Financial Close','Region.fin','Recipient Subregion','Construction', 'EPC Company', \\\n",
    "             'Equipment (GENMFR)', 'TurbMFR', 'Boiler Maker', 'Sector Group', 'Sector', 'Status.fin', 'SSSMFR', \\\n",
    "       'OpYEAR', 'Latitude.fin', 'Longitude.fin', 'Location Accuracy', 'Subnational unit (province, state)',\\\n",
    "            'Sponsor', 'Parent']\n",
    "for x in list_drop_b:\n",
    "    df_both.drop(columns=x,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_platts = pd.merge(df_both, df_only_matched_units_order_short, on='TrackerID', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_platts.to_csv('df_unit_transactions_merged_GCPT_'+pz+'_GCFT_'+fz+'_urg_'+uz+'_platts_'+tz+'_20210729.csv', \\\n",
    "                                      sep=';', encoding='latin-1', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 change order of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_platts = pd.read_csv('df_unit_transactions_merged_GCPT_'+pz+'_GCFT_'+fz+'_urg_'+uz+'_platts_'+tz+'_20210729.csv', \\\n",
    "                                      sep=';', encoding='latin-1', decimal=',', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change order of columns\n",
    "list_columns_all = ['Transaction Nr', 'ownership', 'data','Financing Country', 'Institution', 'Type short', 'Project Name', \\\n",
    "        'matching confidence', 'Comment', 'MW', 'Deflation_Factor_2020', \\\n",
    "        'Amount (in USD 2020)_mio', 'Amount (in USD)', 'Year of Close', 'Recipient Country', 'Phase ID', \\\n",
    "        'number_of_units_matched', 'number_of_trans_matched', 'weighted_transaction_amount_mio', \\\n",
    "       'mean_weighted_transaction_amount_mio', 'total_funding_amount_estimated_mio', 'funding_share', \\\n",
    "       'capacity_weighted_by_funding_share_MW','weighted_funding_amount_estimated_mio', \\\n",
    "                'TrackerID', 'TrackerLOC', \\\n",
    "        'Country', 'Unit', 'Plant', 'Chinese Name', 'Other names', 'Capacity (MW)', 'Status',  \\\n",
    "       'Year', 'RETIRED', 'Plant age', 'Planned Retire', 'Combustion technology', 'Coal type', \\\n",
    "       'Coal source', 'Location','Heat rate', 'Emission factor', \\\n",
    "       'Annual CO2 (million tonnes / annum)', \\\n",
    "       'Lifetime CO2', \\\n",
    "       'UNIT name Platts', 'PLANT name Platts', 'UNITID Platts', 'Matching Confidence', \\\n",
    "           'Sponsor_1', 'Sponsor_1_Country', 'Sponsor_2', 'Sponsor_2_Country', 'Sponsor_3', 'Sponsor_3_Country', \\\n",
    "        'Sponsor_4', 'Sponsor_4_Country', \\\n",
    "        'Parent_1', 'Parent_1_Country', 'Parent_2', 'Parent_2_Country', 'Parent_3', 'Parent_3_Country', \\\n",
    "        'Parent_4', 'Parent_4_Country', 'Parent_5', 'Parent_5_Country', 'Parent_6', 'Parent_6_Country', \\\n",
    "        'Insurer_1', 'Insurer_1_Country', 'Insurer_2', 'Insurer_2_Country', \\\n",
    "        'TURBMFR_1', 'TURBMFR_1_Country', 'TURBMFR_2', 'TURBMFR_2_Country', 'GENMFR_1', 'GENMFR_1_Country', \\\n",
    "        'GENMFR_2', 'GENMFR_2_Country', 'PARTMFR_1','PARTMFR_1_Country', \\\n",
    "       'FGDMFR_1', 'FGDMFR_1_Country', 'NOXMFR_1', 'NOXMFR_1_Country', 'SSSMFR_1', 'SSSMFR_1_Country', \\\n",
    "        'AE_1',  'AE_1_Country', 'AE_2', 'AE_2_Country', 'AE_3', 'AE_3_Country', 'CONSTRUCT_1','CONSTRUCT_1_Country', \\\n",
    "        'CONSTRUCT_2', 'CONSTRUCT_2_Country', 'CONSTRUCT_3',  'CONSTRUCT_3_Country', 'CONSTRUCT_4',  'CONSTRUCT_4_Country']\n",
    "\n",
    "df_both_platts_order = df_both_platts[list_columns_all]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.10 Export final unit-transaction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_platts_order.to_csv('df_order_unit_transactions_merged_GCPT_'+pz+'_GCFT_'+fz+'_urg_'+uz+'_platts_'+tz+'_20210729.csv', \\\n",
    "                                      sep=';', encoding='latin-1', decimal=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
